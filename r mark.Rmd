---
title: "Devoir 2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
```
#Partie théorique
##T-1
### #1
On veut montrer que $\bar{e}=\sum_{i=1}^{n}\frac{e_i}{n}=0$.  On peut donc développer la formule de $\bar{e}$:

\begin{align*}
\bar{e} &= \sum_{i=1}^{n}\frac{e_i}{n}=\sum_{i=1}^{n}\frac{Y_i-\hat{Y_i}}{n}\\
        &=\bar{Y} -\frac{\sum_{i=1}^{n}x_{i}^{'}\hat{\beta}}{n}   \\
\end{align*}

Nous pouvons écrire la somme de droite de cette façon:

\begin{align*}
\sum_{i=1}^{n}x_{i}^{'}\hat{\beta}&=\hat{\beta}_0\sum_{i=1}^{n}1 + \hat{\beta}_1\sum_{i=1}^{n}x_{i,1}+...+\hat{\beta}_{p^{'}}\sum_{i=1}^{n}x_{i,p^{'}} \\
\frac{\sum_{i=1}^{n}x_{i}^{'}\hat{\beta}}{n} &= \hat{\beta}_0 +\hat{\beta}_1 \bar{x}_1 +...+\hat{\beta}_{p^{'}} \bar{x}_{p^{'}} 
\end{align*}

Tel que donnée dans l'énoncé, on peut prendre pour acquis que:  $$\hat{\beta_0}=\bar{Y}-\hat{\beta_1}\bar{x}_1-...-\hat{\beta_{p^{'}}}\bar{x}_{p^{'}}$$

Ainsi, en remplaçant les valeurs obtenues avec l'équation plus haut, on obtient:

\begin{align*}
\bar{e}&=\bar{Y} -\frac{\sum_{i=1}^{n}x_{i}^{'}\hat{\beta}}{n}   \\
&=\bar{Y}-\left(\hat{\beta}_0 +\hat{\beta}_1 \bar{x}_1 +...\hat{\beta}_{p^{'}} \bar{x}_{p^{'}}\right)\\
&=\bar{Y}-\left(\left[\bar{Y}-\hat{\beta_1}\bar{x}_1-...-\hat{\beta_{p^{'}}}\bar{x}_{p^{'}}\right] +\hat{\beta}_1 \bar{x}_1 +...+\hat{\beta}_{p^{'}} \bar{x}_{p^{'}}\right)\\
&=\bar{Y}-\bar{Y}\\
\bar{e}&=0
\end{align*}


### #4
Pour faire la courbe ROC, il faut calculer la valeur de $\hat{\pi}_i=P(Y_i=1,x_i)=\frac{exp(-4.2+1.2x_i)}{1+exp(-4.2+1.2x_i)}$. Par la suite on calcule une prédiction de $\hat{Y}_i$ selon un certain seuil $u_k$ : si $\hat{\pi}_i>u_k$, alors $\hat{Y}_i=1$, sinon $\hat{Y}_i=0$. On calule alors la sensibilié et la spécificité pour les différents seuils. Voici un tableau des résulats ($\hat{Y}_i$ est représenté comme étant $\hat{Y}_{u}$ selon la valeur de $u_k$) pour les valeurs de $u_k$ =0.1, 0.2, 0.5, 0.8 et 0.9 ainsi que le graphique de la courbe ROC:

```{r theorie1_4,echo=FALSE}
library(knitr)

f=function(x){
  ni=-4.2+1.2*x
  1/(1+exp(-ni))
}


mathy.df <- data.frame(
  labels=c("obs 1","obs 2","obs 3","obs 4","obs 5","obs 6"),
  xi = c(1:6), 
  prob_i=c(f(1:6)),
  Y=c(0,0,1,0,1,1),
  Y_u1=c(0,1,1,1,1,1),
  Y_u2=c(0,0,1,1,1,1),
  Y_u3=c(0,0,0,1,1,1),
  Y_u4=c(0,0,0,0,1,1),
  Y_u5=c(0,0,0,0,0,1)

                       )

colnames(mathy.df) <- c("Observation","$x_i$", "$\\hat{\\pi}_i$","$Y_i$",
                        "$\\hat{Y}_{u=0.1}$",
                        "$\\hat{Y}_{u=0.2}$","$\\hat{Y}_{u=0.5}$",
                        "$\\hat{Y}_{u=0.8}$","$\\hat{Y}_{u=0.9}$")

df_vp=data.frame(
  labels=c("VP","FN","VN","FP","Sensibilité","Spécificité"),
  y1=c(3,0,1,2,1,1/3),
  y2=c(3,0,2,1,1,2/3),
  y3=c(2,1,2,1,2/3,2/3),
  y4=c(2,1,3,0,2/3,1),
  y5=c(1,2,3,0,1/3,1)
                )

colnames(df_vp) <- c("Métriques","$\\hat{Y}_{u=0.1}$",
                        "$\\hat{Y}_{u=0.2}$","$\\hat{Y}_{u=0.5}$",
                        "$\\hat{Y}_{u=0.8}$","$\\hat{Y}_{u=0.9}$")

kable(mathy.df, escape=FALSE)

kable(df_vp, escape=FALSE)


#Graphique
sens=c(1,1,2/3,2/3,1/3)
spec=c(1/3,2/3,2/3,1,1)

plot(1-spec,sens,type="b",main="Courbe ROC",xlab="1-Spécificité",ylab="Sensibilité", ylim=c(0,1),col="blue")

```




\clearpage

##T-2

### #4

Le modèle est défini de la façon suivante:
$$Y_{ij}=\beta_0 +\gamma_{i0}+(\beta_1+\gamma_{i1})x_{ij}+\epsilon_{ij}$$
On peut définir la matrice de betas et de gammas pour nos tests d'hypothèse qui prendront tous la même forme:
$$\left[\begin{array}
{r}
\boldsymbol{\beta}\\
\boldsymbol{\gamma}
\end{array}\right]
 = \left[\begin{array}
{r}
\beta_0\\
\beta_1\\
\gamma_{10}\\
\gamma_{11}\\
\gamma_{20} \\
\gamma_{21} \\
\end{array}\right]
$$


$$H_0:\boldsymbol{L}  \left[\begin{array}
{r}
\boldsymbol{\beta} \\
\boldsymbol{\gamma} 
\end{array}\right]=\boldsymbol{d}
$$
$$H_1:\boldsymbol{L}  \left[\begin{array}
{r}
\boldsymbol{\beta} \\
\boldsymbol{\gamma} 
\end{array}\right]\ne \boldsymbol{d}
$$

####a)
Pour tester si l’effet moyen du médicament sur la pression sanguine moyenne dans cette population de souris est nul, on doit regarder l'espérance de $Y$ lorsque les effets aléatoires sont nuls. Alors,

$$E[Y ; x_0 = x+1] - E[Y ; x_0 = x] = 0$$
Ce qui revient à tester si $\beta_1 = 0$.

$$\boldsymbol{L}=  \left[\begin{array}
{rrrrrr}
0&1&0&0&0&0\\
\end{array}\right]
$$
$$\boldsymbol{d}=  \left[\begin{array}
{r}
0\\
\end{array}\right]
$$

####b)
Pour tester si l’effet du médicament sur la pression sanguine moyenne dans la famille 1 est nul, ceci revient à tester si la valeur devant $x_{1j}$ est nulle pour la famille 1. Ceci revient alors à tester si

$$\beta_1 +\gamma_{11}=0 $$
$$\boldsymbol{L}=  \left[\begin{array}
{rrrrrr}
0&1&0&1&0&0\\
\end{array}\right]
$$
$$\boldsymbol{d}=  \left[\begin{array}
{r}
0\\
\end{array}\right]
$$

####c)
Afin de tester si l’effet du médicament sur la pression sanguine moyenne est nul chez chacune des deux familles de souris, alors on souhaite tester si les valeurs devant $x_{ij}$ pour les 2 familles sont nulles. Alors, on peut tester simulatément ceci:
$$\beta_1 +\gamma_{11}=0 \text{ et } \beta_1 +\gamma_{21}=0$$
$$\boldsymbol{L}=  \left[\begin{array}
{rrrrrr}
0&1&0&1&0&0\\
0&1&0&0&0&1\\
\end{array}\right]
$$
$$\boldsymbol{d}=  \left[\begin{array}
{r}
0\\
0\\
\end{array}\right]
$$

####d)
Afin de tester si une hausse d’une unité de la dose de médicament dans la famille 1 est équivalente à une hausse d’une unité de la dose de médicament dans la famille 2 en termes d’effet sur la pression sanguine moyenne, on peut réécrire le tout de la manière suivante:
$$E[Y_1;x^{*}_{10}=x_{10}+1]-E[Y_1;x^{*}_{10}=x_{10}]=E[Y_2;x^{*}_{20}=x_{20}+1]-E[Y_2;x^{*}_{20}=x_{20}]$$

Alors ceci revient à tester si 
$$\beta_1 + \gamma_{11} = \beta_1 + \gamma_{21}$$
$$\gamma_{11} - \gamma_{21} = 0$$

$$\boldsymbol{L}=  \left[\begin{array}
{rrrrrr}
0&0&0&1&0&-1\\
\end{array}\right]
$$
$$\boldsymbol{d}=  \left[\begin{array}
{r}
0\\
\end{array}\right]
$$



### #5
####a) 
On peut définir le modèle de cette façon: $Y_{ij}=\beta_0+\gamma_{i0} +\beta_1x_{ij} +\epsilon_{ij}$. Voici la notation matricielle:

$$\mathbf{Y'} = \left[\begin{array}
{rrrrrr}
Y_{11}&Y_{12}&Y_{21}&Y_{22}&Y_{31}&Y_{32}\\
\end{array}\right]
$$
$$\mathbf{X} = \left[\begin{array}
{rr}
1 & x_{11}\\
1 & x_{12}\\
1 & x_{21}\\
1 & x_{22}\\
1 & x_{31}\\
1 & x_{32}\\
\end{array}\right]
$$
$$\boldsymbol{\beta} = \left[\begin{array}
{r}
\beta_0\\
\beta_1
\end{array}\right]
$$
$$\boldsymbol{\gamma} = \left[\begin{array}
{r}
\gamma_{10}\\
\gamma_{20}\\
\gamma_{30}
\end{array}\right]
$$
$$\mathbf{Z} = \left[\begin{array}
{rrr}
1 &  0 & 0\\
1 &  0 & 0\\
0 &  1 & 0\\
0 &  1 & 0\\
0 &  0 & 1\\
0 &  0 & 1\\
\end{array}\right]
$$
$$\mathbf{\epsilon'} = \left[\begin{array}
{rrrrrr}
\epsilon_{11}&\epsilon_{12}&\epsilon_{21}&\epsilon_{22}&\epsilon_{31}&\epsilon_{32}
\end{array}\right]
$$
On peut également remplacer les valeurs symboliques de \textbf{Y} et \textbf{X} par leurs valeurs numériques:

$$\mathbf{Y'} = \left[\begin{array}
{rrrrrr}
70&80&50&60&100&70
\end{array}\right]
$$
$$\mathbf{X} = \left[\begin{array}
{rr}
1 & 1\\
1 & 0\\
1 & 1\\
1 & 0\\
1 & 0\\
1 & 1\\
\end{array}\right]
$$


####b)

On trouve les matrices de variance:


$$\mathbf{D}=Var(\boldsymbol{\gamma}) = \left[\begin{array}
{rrr}
\sigma_0^{2} &  0 & 0\\
0 &  \sigma_0^{2} & 0\\
0 &  0 & \sigma_0^{2}\\
\end{array}\right]
$$
$$\mathbf{V}=Var(\boldsymbol{\epsilon}) = \left[\begin{array}
{rrrrrr}
\sigma^{2}&0&0&0&0&0\\
0&\sigma^{2}&0&0&0&0\\
0&0&\sigma^{2}&0&0&0\\
0&0&0&\sigma^{2}&0&0\\
0&0&0&0&\sigma^{2}&0\\
0&0&0&0&0&\sigma^{2}\\
\end{array}\right]
$$
$$\mathbf{\Sigma}=Var(\mathbf{Y})=\mathbf{Z}\mathbf{D}\mathbf{Z}^{'}+\mathbf{V}$$

$$\mathbf{\Sigma} = \left[\begin{array}
{rrrrrr}
\sigma^{2}+\sigma_0^{2}&\sigma_0^{2}&0&0&0&0\\
\sigma_0^{2}&\sigma^{2}+\sigma_0^{2}&0&0&0&0\\
0&0&\sigma^{2}+\sigma_0^{2}&\sigma_0^{2}&0&0\\
0&0&\sigma_0^{2}&\sigma^{2}+\sigma_0^{2}&0&0\\
0&0&0&0&\sigma^{2}+\sigma_0^{2}&\sigma_0^{2}\\
0&0&0&0&\sigma_0^{2}&\sigma^{2}+\sigma_0^{2}\\
\end{array}\right]
$$




####c)
Ces deux valeurs peuvent s'estimer avec les formules suivantes:

$$\boldsymbol{\hat{\beta}}=(\mathbf{X^{'}\Sigma^{-1}X})^{-1}\mathbf{X^{'}\Sigma}^{-1}\mathbf{Y} $$
$$\boldsymbol{\hat{\gamma}}=\mathbf{DZ^{'}\Sigma}^{-1}(\mathbf{Y}-\boldsymbol{X\beta})   $$
On peut utiliser R pour calculer les valeurs numériques de ces estimés:


```{r theorie2_5,echo=FALSE,eval=FALSE}
library(lme4)
Z=matrix(c(1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1),ncol=3)

sig0=2
D=diag(x=sig0,ncol=3,nrow=3)

sig=1
V=diag(x=sig,ncol=6,nrow=6)

Y=matrix(c(70,80,50,60,100,70),ncol=1)
X=matrix(c(1,1,1,1,1,1,1,0,1,0,0,1),ncol=2)

mat_Sig=Z%*%D%*%t(Z)+V


p1_B=t(X)%*%solve(mat_Sig)%*%X
p2_B=t(X)%*%solve(mat_Sig)
B=solve(p1_B)%*%p2_B%*%Y

Gam=D%*%t(Z)%*%solve(mat_Sig)%*%(Y-X%*%B)

Z
D
V
Y
X
mat_Sig
B
Gam

```


$$\boldsymbol{\hat{\beta}} = \left[\begin{array}
{r}
80\\
-16.67
\end{array}\right]
$$
$$\boldsymbol{\hat{\gamma}} = \left[\begin{array}
{r}
2.67\\
-13.33\\
10.67\\
\end{array}\right]
$$
 
#### d)

On peut calculer l'estimé à partir de cette équation:
$$\boldsymbol{\hat{Y_i}} =\boldsymbol{V_i\Sigma_i}^{-1}\boldsymbol{X_i^{'}\hat{\beta}}+(\boldsymbol{I}_{n_i*n_i}-\boldsymbol{V_i\Sigma_i}^{-1})\boldsymbol{Y_i} $$
```{r theorie2_5_d,echo=FALSE,eval=FALSE}
Z=matrix(c(1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1),ncol=3)

sig0=2
D=diag(x=sig0,ncol=3,nrow=3)

sig=1
V=diag(x=sig,ncol=6,nrow=6)

Y=matrix(c(70,80,50,60,100,70),ncol=1)
X=matrix(c(1,1,1,1,1,1,1,0,1,0,0,1),ncol=2)

mat_Sig=Z%*%D%*%t(Z)+V


p1_B=t(X)%*%solve(mat_Sig)%*%X
p2_B=t(X)%*%solve(mat_Sig)
B=solve(p1_B)%*%p2_B%*%Y

Gam=D%*%t(Z)%*%solve(mat_Sig)%*%(Y-X%*%B)

Z
D
V
Y
X
mat_Sig
B
Gam
solve(mat_Sig)

mat_Sig2_inv=matrix(c(0.6,-0.4,-0.4,0.6),ncol=2)
V2=diag(x=sig,ncol=2,nrow=2)
Y2=matrix(c(50,60),ncol=1)
I2=diag(x=1,ncol=2,nrow=2)
X2=matrix(c(1,1,1,1),ncol=2)

pred=V2%*%mat_Sig2_inv%*%X2%*%B+(I2-V2%*%mat_Sig2_inv)%*%Y2
V2%*%mat_Sig2_inv
X2%*%B
(I2-V2%*%mat_Sig2_inv)
Y2
pred

```
Où:

$$\boldsymbol{V_i\Sigma_i}^{-1}= \left[\begin{array}
{rr}
0.6 &  -0.4 \\
-0.4 &  0.6 \\
\end{array}\right]
$$


De cette façon, on obtient la valeur moyenne de $\boldsymbol{\hat{Y_i}} =56.67$ pour notre estimé de la note moyenne obtenue par l’individu 2 dans les cours où il utilise un manuel de langue anglaise.


### #6

####a)
Dans cette situation, le paramètre d'intérêt est $\beta_3$ puisque celui-ci estimera l'effet sur la valeur de $Y_{ij}$ au fil du temps seulement lorsque $x_i=1$.

####b)
Il serait raisonnable de choisir les strutures AR(1) et UN(1) puisque cette paire a la plus petite valeur d'AIC pour la méthode ML.

####c)
On procède à un test d'hypothèse formel en testant un modèle complet (ligne 1) et un modèle réduit sans la pente aléatoire (ligne 2):

$$H_0: Y_{ij}=\beta_0+\gamma_{i0}+\beta_1x_i +\beta_2t_j+\beta_3x_it_j+\epsilon_{ij} $$
$$H_1: Y_{ij}=\beta_0+\gamma_{i0}+\beta_1x_i +(\beta_2+\gamma_{i2})t_j+\beta_3x_it_j+\epsilon_{ij} $$

On pose : $\epsilon=2(\ell_1-\ell_0)=2(-88+89.5)=3$. (On pourrait également utiliser les mesures REML pour des résultats semblables).

Nous rejeterons l'hypothèse si la p-value du test est trop élevée:

$$p value=0.5P[\chi^{2}_{m_1-m_0-1}>\epsilon]+0.5P[\chi^{2}_{m_1-m_0}>\epsilon]$$
$m_0=1$(1 variance) et $m_1=2$(2 variances). Par conséquent:

$$p=0.5P[\chi^{2}_{0}>\epsilon]+0.5P[\chi^{2}_{1}>\epsilon]=0+0.5*0.08326=0.04163226$$
La p-value est inférieur à 0.05, par conséquent on rejète l'hypothèse du modèle réduit avec ce seuil de 5% et on conserve l'effet aléatoire $\gamma_{i2}$. Il est toutefois à noter que la p-value est assez proche de 0.05 avec une valeur de 0.0416.


\clearpage


#Partie pratique
##P-1
On reprend le modèle complet du TP1, sans la variable *LG9*, car il y avait de la multicolinéarité en sa présence.
```{r import,echo=FALSE, include=FALSE}
library(olsrr)
library(ggplot2)
library(car)
library(MASS)
library(stats)
library(glmbb)
library(glmnet)
library(plotmo)  
library(xts)
library(sp)
library(CASdatasets)
data_tp1=read.table("weisberg56.dat",header=TRUE)
modele_complet=lm(SOMA ~ WT2+HT2+WT9+HT9+ST9, data = data_tp1)

si <- studres(modele_complet) # residus studentises
hatYi <- modele_complet$fitted.values # valeurs ajustees
i <- 1:length(data_tp1$SOMA)

```

Commençons par regarder les graphiques de résidus du modèle complet. 
```{r fig1,echo=FALSE, fig.height = 3, fig.width = 3, fig.align = "center"}
# Graphique des paires (hatYi,ei)
ols_plot_resid_fit(modele_complet)
```
```{r fig1.1,echo=FALSE, fig.height = 3, fig.width = 3}
plot(hatYi,si,xlab="valeurs ajustées",ylab="résidus studentisés")
abline(h=0,lty=2)
# Graphique des paires (i,si)
plot(i,si,xlab="i",ylab="si")
abline(h=0,lty=2)

```
$\\ \\$
L'allure des résidus semble correct. Il y a peut-être un petit manque de linéarité. Le test de *Box-cox* va être un bon indicateur si on doit faire une transformation sur la variable endogène. De plus on pourrait tester quelques intéractions. Comme on l'indiquait lors de TP1, plus un enfant de 2ans est pesant, mois son indice d'obésité à 18ans sera élevé. Cela peut sembler contre intuitif. Peut-être que la grandeur de l'enfant est vraiment importante à 2ans pour avoir une bonne mesure du poids. On va donc tester d'ajouter un intéraction entre le poids et la grandeur de l'enfant à 2ans. 

```{r fig2,echo=FALSE, fig.height = 3, fig.width = 3}
# QQ-plot normal
ols_plot_resid_qq(modele_complet)
# Tests de normalite, incluant Shapiro-Wilk
ols_test_normality(modele_complet)
```
Le graphique de *QQ-plot* montre que les résidus semblent être distribué de façon normal. De plus le test de *Shapiro-Wilk* fait en sorte qu'on ne rejette pas l'hypothèse de normalité des résidus.

Regardons le graphique des valeurs de lambda avec la transformation de *Box-Cox*
$\\ \\$
```{r fig3,echo=FALSE, fig.height = 3, fig.width = 3}
# Transformation de Box-Cox
boxcox(modele_complet)
```
$\\ \\$
Il serait approprié de faire notre modèle de régression linéaire sur $\sqrt{Y}$. 
$\\ \\$
Regardons maintenant certaines statistiques sur l'influence des données.
$\\ \\$
```{r influence1,echo=FALSE}
# DFFITS
ols_plot_dffits(modele_complet)
```

```{r influence1.1,echo=FALSE}
# Residus vs h_ii
ols_plot_resid_lev(modele_complet)
```
$\\ \\$
Les données 18 et 20 peuvent potentiellement causer des problèmes. Ils ont une certaines influences sur les valeurs prédites. Cependant, avec le graphique des résidus en fonction des $h_{ii}$ on remarque que ces données sont des outliers, mais cependant il n'ont pas un grand effet de levier. Donc il est correct de les conserver pour faire le modèle.
$\\ \\$
```{r influence1.3,echo=FALSE}
# tableau résumé
influence.measures(modele_complet)
```
En regardant la distance de cook ainsi que le  *covratio*, on remarque que certaine données influence la valeur des *beta* et la variance des estimateurs. Comme nous avons que 26 données, nous allons tout de même les conserver dans le modèle.

On reprend maintenant le modèle final du TP1 avec les variables  *WT2, WT9, HT9 & ST9*
Commençons par regarder les graphiques de résidus du modèle choisi. 
```{r fig4,echo=FALSE, fig.height = 3, fig.width = 3, fig.align = "center"}
modele_choisi=lm(SOMA ~ WT2+WT9+HT9+ST9, data = data_tp1)

si <- studres(modele_choisi) # residus studentises
hatYi <- modele_choisi$fitted.values # valeurs ajustees
i <- 1:length(data_tp1$SOMA)
# Graphique des paires (hatYi,ei)
ols_plot_resid_fit(modele_choisi)
```

```{r fig4.1,echo=FALSE, fig.height = 3, fig.width = 3}
plot(hatYi,si,xlab="valeurs ajustées",ylab="résidus studentisés")
abline(h=0,lty=2)
# Graphique des paires (i,si)
plot(i,si,xlab="i",ylab="si")
abline(h=0,lty=2)

```
$\\ \\$
L'allure des résidus semble correct. Les graphiques ont pratiquement la même allure que pour le modèle complet. Les mêmes conclusions sont tirées.

```{r fig5,echo=FALSE, fig.height = 3, fig.width = 3}
# QQ-plot normal
ols_plot_resid_qq(modele_choisi)
# Tests de normalite, incluant Shapiro-Wilk
ols_test_normality(modele_choisi)
```
Le graphique de *QQ-plot* montre que les résidus semblent être distribué de façon normal. De plus le test de *Shapiro-Wilk* fait en sorte qu'on ne rejette pas l'hypothèse de normalité des résidus.

Regardons le graphique des valeurs de lambda avec la transformation de *Box-Cox*
$\\ \\$
```{r fig6,echo=FALSE, fig.height = 3, fig.width = 3}
# Transformation de Box-Cox
boxcox(modele_choisi)
```
$\\ \\$
Il serait aussi approprié de faire notre modèle de régression linéaire sur $\sqrt{Y}$. 

$\\ \\$
Regardons maintenant certaines statistiques sur l'influence des données pour le modèle final.
$\\ \\$
```{r influence2,echo=FALSE}
# DFFITS
ols_plot_dffits(modele_choisi)
```

```{r influence2.1,echo=FALSE}
# Residus vs h_ii
ols_plot_resid_lev(modele_choisi)
```
$\\ \\$
Les données 18 et 20 peuvent potentiellement causer des problèmes. Ils ont une certaines influences sur les valeurs prédites. Cependant, avec le graphique des résidus en fonction des $h_{ii}$ on remarque que ces données sont des outliers, mais cependant il n'ont pas un grand effet de levier. Donc il est correct de les conserver pour faire le modèle.
$\\ \\$
```{r influence2.3,echo=FALSE}
# tableau résumé
influence.measures(modele_choisi)
```
En regardant la distance de cook ainsi que le  *covratio*, on remarque que certaines données influencent la valeur des *beta* et la variance des estimateurs. Comme nous avons que 26 données, nous allons tout de même les conserver dans le modèle.



Le modèle final est celui où l'on effectue la régression sur $\sqrt{Y}$. De plus on ajoute la variable *HT2* ainsi que l'intéraction entre *WT2 et HT2* dans le modèle. Nous avons maintenant une relation positive entre le poids à 2ans et l'indice d'obésité. Voici maintenant la prédiction du modèle.
$\\ \\$
```{r final,echo=FALSE, fig.height = 3, fig.width = 3, fig.align = "center"}
modele_final=lm(sqrt(SOMA) ~ WT2+HT2+WT9+HT9+ST9+WT2*HT2, data = data_tp1)
vecteur_data=data.frame(WT2=13,HT2=90,WT9=41,HT9=141,ST9=73)
print("Prédiction pour Y")
predict(modele_final,newdata = vecteur_data,interval=c("prediction"),level=0.95)^2
```
Un graphique intéressant à regarder est les résidus en fonction de $h_{ii}$. Il n'y a maintenant plus d'observation qui sont vraiment outlier. 
$\\ \\$
```{r influence3.1,echo=FALSE}
ols_plot_resid_lev(modele_final)
```
\newpage
##P-2

Tout d'abord, il est important de commencer l'analyse en considérant les effets aléatoires possibles dans le modèle. Puisque les variables `ratio` ainsi que `meanses` ne varient pas en fonction de l'école, ces variables ne sont pas utilisable comme pente aléatoire dans le modèle. Par ailleurs, il serait probable que le nombre d'heures de travail de l'étudiant à la maison par semaine varient selon l'école. Afin de constaté ceci, il est possible de faire un modèle complet de régression standard et visualiser sur un graphique les résidus en fonction de la variable `homework` pour ces différentes écoles. Afin d'avoir une vue allégée du graphique, seulement les 5 premières écoles sont dans le graphique des résidus plus bas.

```{r graph_residus,echo=FALSE,eval=TRUE}

data.tp2 = read.table("homework.txt",header=TRUE)


data.clean=data.tp2[,c("schid","meanses","homework","white","ratio","math")]


modele.simple = lm(math~meanses+homework+white+ratio+meanses*white,data=data.clean)
resid.lm = residuals(modele.simple)

#Logiquement, on peut se dire que la seule pente aléatoire pouvant être incluse
#dans le modèle est celle devant le nombre d'heure. Allons voir ceci.
some_obs=unique(data.clean$schid)[1:5]


plot(data.clean$homework[data.clean$schid==some_obs[1]],resid.lm[data.clean$schid==some_obs[1]],col=some_obs[1],
     type="p",pch=20,xlab="homework",ylab="Résidus",ylim=c(-7,7),
     main="Résidus de la régression ordinaire")
# Boucle pour tracer les segments pour les rats suivants
for(i in 2:5){
  points(data.clean$homework[data.clean$schid==some_obs[i]],resid.lm[data.clean$schid==some_obs[i]],col=some_obs[i],pch=20)
}
#juste avec cela, on remarque qu'on a fort probablement besoin d'un effet aléatoire devant homework


```

Bien qu'il soit difficile à analyser, on semble voir une certaine dépendance avec l'école.


# (a)
Commençons en créant un modèle linéaire mixte sans considérer la variable `meanses`.

Alors, avec le modèle suivant:
$$Y_{ij}=\beta_0+\gamma_{i0}+\beta_1x_{ij1}+\beta_2x_{i2} +(\beta_3+\gamma_{i3})x_{ij3}+\epsilon_{ij} $$
où  
$Y_{ij}$ = résultat de l'étudiant j dans l'école i en mathématique.  
$x_{ij1}$ = l'indicateur de 1 pour blanc pour l'étudiant j dans l'école i.  
$x_{i2}$ = est le ratio du nomde d'étudiants par enseignant dans les classes de l'école i.  
$x_{ij3}$ = le nombre d'heures de travail à la maison de l'étudiant j dans l'école i.  
 
on peut tester si on obtient un AIC minimal avec les blocs de la matrice D non-structuré (UN) ou diagonale principale (UN(1)). Par simplicité et limitation du package `lme4`, la matrice V sera de composante de variance (VC).


```{r AIC_,echo=FALSE,eval=TRUE}
## Allons voir avec le modèle mixte. ##
# Option 1: VC pour V, UN pour D
ML.VCUN = lmer(math~white+ratio+homework
               +(homework|schid),
               data=data.clean,REML = FALSE)
ML.VCUN1 = lmer(math~ratio+white+homework
                +(homework||schid),data=data.clean,REML=FALSE)

xyz=matrix(c(extractAIC(ML.VCUN)[2],extractAIC(ML.VCUN1)[2]),nrow=1)
colnames(xyz)=c("VC et UN","     VC et UN(1)")
rownames(xyz)="AIC"
xyz

```


Alors, on choisit le modèle avec les blocs de la matrice D non-structuré (UN).  

On peut poursuivre en effectuant un test de la nécessité des effets aléatoires avec le test suivant:
$$H_0: Y_{ij}=\beta_0+\gamma_{i0}+\beta_1x_{ij1}+\beta_2x_{i2} +\beta_3x_{ij3}+\epsilon_{ij}$$
$$H_1: Y_{ij}=\beta_0+\gamma_{i0}+\beta_1x_{ij1}+\beta_2x_{i2} +(\beta_3+\gamma_{i3})x_{ij3}+\epsilon_{ij}$$

Et nous pourrons obtenir la p-value avec cette équation:

$$p=0.5P[\chi^{2}_{1}>\epsilon]+0.5P[\chi^{2}_{2}>\epsilon]$$
puisque nous avons que $m_0=1$(1 variance) et $m_1=3$(2 variances et 1 covariance). On obtient une valeur de $\epsilon=2(\ell_1-\ell_0)=89.74556$ et on obtient une p-value très près de 0. Alors, on rejette le l'hypothèse nulle et nous ne pouvons pas simplifier le modèle.

```{r Test_nec,echo=FALSE,eval=FALSE}

#On poursuit avec le test de la nécessite des effets aléatoires.
#Ho avec 1 effet aléatoire
#H1 avec 2 effet2 aléatoires
ML.VCUN_H0 = lmer(math~white+ratio+homework
                  +(1|schid),
                  data=data.clean,REML = FALSE)


epsilon = 2*(logLik(ML.VCUN)-logLik(ML.VCUN_H0))

#on obtient un epsilon de 89.74556
#puisque nous avons la matrice UN pour D, alors, sous H0 on a une variance,
#et sous H1 on a 3 paramètres (2 variances, une covariance).
pval = 0.5*(1-pchisq(89.74556,2))+0.5*(1-pchisq(89.74556,1))
names(pval) = "p-value"


```

Il est possible d'effectuer une sélection des effets fixes avec la méthodes backward, où nous rejetons la variable `ratio`.


```{r effets_fixes,echo=FALSE,eval=TRUE}
#passons maintenant aux effets fixes.
library(car)
#Avant tout, assurons-nous d'avoir la méthode REML
ML.VCUN = lmer(math~white+ratio+homework
               +(homework|schid),
               data=data.clean,REML = TRUE)
ann=Anova(ML.VCUN)
# On rejette la variable ratio
ML.VCUN_Iter2 = lmer(math~white+homework
                     +(homework|schid),
                     data=data.clean,REML = TRUE)
ann=Anova(ML.VCUN_Iter2)


#On obtient ensuite nos betas et gamma ainsi
#coef(ML.VCUN_Iter2)
print("Résumé des coefficients des effets fixes")
coef(summary(ML.VCUN_Iter2))[ , "Estimate"]

print("Résumé des effets aléatoires")
ranef(ML.VCUN_Iter2)

```

Alors, pour répondre aux questions de l'énoncé (a), l'effet du nombre d'heure est positif, alors plus un étudiant passe d'heure d'étude par semaine, meilleur sont ses résultats en mathématiques. De plus, l'augmentation moyenne sur le résultat en mathématique d'une heure d'étude supplémentaire par semaine sur l'ensemble des étudiant est de $1.903103 (\hat{\beta_3})$. Par ailleurs, puisque nous avons rejeté le modèle sans la pente aléatoire devant la variable $x_{ij3}$, on peut conclure qu'il est raisonnable de croire que l'effet du nombre d'heures varient d'une école à une autre.



# (b)
En procédant de la même façon que les étapes en (a) et en y rajoutant la variable `meanses`, on conserve la structure des variances VC et UN puisqu'on obtient un AIC minimal. De plus, en effectuant le test de la nécessité des effets aléatoires, on obtient un résultat identique, c'est-à-dire qu'on conserve la pente aléatoire devant la variable $x_{ij3}$ (nombre d'heures de travail par semaine tel que défini plus haut).




```{r num_b,echo=FALSE,eval=FALSE}


#b) AVEC la variables meanses
## Allons voir avec le modèle mixte. ##
# Option 1: VC pour V, UN pour D
ML.VCUN = lmer(math~white+ratio+meanses+homework+meanses*homework
               +(homework|schid),
               data=data.clean,REML = FALSE)
extractAIC(ML.VCUN)
ML.VCUN1 = lmer(math~ratio+white+meanses+homework+meanses*homework
                +(homework||schid),data=data.clean,REML=FALSE)
extractAIC(ML.VCUN1)
# On choisit VC et UN puisqu'on a le plus petit AIC avec ces choix de structure


#On poursuit avec le test de la nécessite des effets aléatoires.
#Ho avec 1 effet aléatoire
#H1 avec 2 effets aléatoires
ML.VCUN_H0 = lmer(math~white+ratio+homework+meanses+meanses*homework
                  +(1|schid),
                  data=data.clean,REML = FALSE)


epsilon = 2*(logLik(ML.VCUN)-logLik(ML.VCUN_H0))

#on obtient un epsilon de 87.34016
#puisque nous avons la matrice UN pour D, alors, sous H0 on a une variance,
#et sous H1 on a 3 paramètres (2 variances, une covariance).
pval = 0.5*(1-pchisq(87.34016,2))+0.5*(1-pchisq(87.34016,1))
pval

#On rejette alors fortement H0 et on conserve le modèle tel quel.

#Alors, oui leffet semble toujours varier par école
```

En utilisant la méthode backward pour la sélection des effets fixes, on rejette, en 2 étapes, la variable `ratio` ainsi que l'interaction `meanses*homework`


```{r num_b2,echo=FALSE,eval=TRUE}

#passons maintenant aux effets fixes.
library(car)
#Avant tout, assurons-nous d'avoir la méthode REML
ML.VCUN = lmer(math~white+ratio+homework+meanses+meanses*homework
               +(homework|schid),
               data=data.clean,REML = TRUE)
ann=Anova(ML.VCUN)
# On rejette la variable ratio
ML.VCUN_Iter2 = lmer(math~white+homework+meanses+meanses*homework
                     +(homework|schid),
                     data=data.clean,REML = TRUE)
ann=Anova(ML.VCUN_Iter2)
#Et on rejette ensuite l'interaction
ML.VCUN_Iter3 = lmer(math~white+homework+meanses
                     +(homework|schid),
                     data=data.clean,REML = TRUE)
ann=Anova(ML.VCUN_Iter3)

#On obtient ensuite nos betas et gamma ainsi
#coef(ML.VCUN_Iter3)
print("Résumé des coefficients des effets fixes")
coef(summary(ML.VCUN_Iter3))[ , "Estimate"]
print("Résumé des effets aléatoires")
ranef(ML.VCUN_Iter3)

```

Alors, pour répondre aux questions du numéro, l'ajout de la variable `meanses` au modèle n'a pas diminué le besoin d'inclure des effets aléatoires. De plus, l'effet moyen sur le résultat en mathématique d'une heure d'étude supplémentaire par semaine sur l'ensemble des étudiants est environ le même qu'obtenu précédemment, soit $\hat{\beta_3} = 1.925085$. Par ailleurs, puisque nous avons rejeté le modèle sans la pente aléatoire devant la variable $x_{ij3}$, on peut conclure qu'il est raisonnable de croire que l'effet du nombre d'heures varient d'une école à une autre.  
Alors, le fait d'inclure ou non une variable unique à chaque école dans le modèle n'a pas réduit l'importance d'avoir des effets aléatoires dans le modèle et n'a non plus pas changé les conclusions.
